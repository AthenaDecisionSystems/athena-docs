{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Athena Decision Systems help customers make better decisions in their day-to-day business by providing Hybrid AI solutions that combine generative AI with business rules and machine learning to deliver high-impact systems.</p> <p>We provide software components and professional services.</p>"},{"location":"index-wip/","title":"Welcome to Athena Decision Systems","text":"<p>PICTURE PROVIDING VISUAL INTRODUCTION HERE (30s)</p> <ul> <li>the problem we address</li> <li>how we solve it</li> <li>how is it different</li> </ul> <p>Athena Decision Systems help customers make better decisions in their day-to-day business by providing Hybrid AI solutions that combine generative AI with business rules and machine learning to deliver high-impact systems.</p> <p>We provide software components and professional services.</p> <p>ENTERPRISE AI IN ACTION Discover how we help enterprises leverage Hybrid AI to build smarter interactive applications (3 min video): New video coming soon </p> <p>To know more about Athena Decision Systems visit athenadecisions.com.</p>"},{"location":"Contribute/","title":"Contribute","text":"<p>Coming soon</p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/","title":"Anatomy of a solution","text":"<p>This chapter presents the high level concepts and constructs of the Owl Agent framework and how it is used in the context of a custom solution.</p> <p>Agent is a deployable unit built up by choreographing one or more llm, each with its own workflow that can leverage external tools, and guided by prompts.</p> <p>A typical solution includes: 1/ a front end to let a human interact with the system, 2/ a backend to manage the life cycle of agents integrated with a LLM running as a service, with dynamic integration to tools and functions, vector store retrievers, and decision services. 3/ Decision service with inference rules to control the decision to be made.</p> <p></p> <p>4/ The conversation is stateful, persisted and integrated with the different agents defined in the solution. </p> <p>The Owl Agent backend may manage multiple concurrent conversations and multiple agents instances. It can scale horizontally too. </p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/#key-concepts","title":"Key concepts","text":"<p>The main concepts the framework defines and uses are:</p> <ul> <li>An Agent manages a co-ordinated set of calls to a Large Language Model,  with a prompt and tools, to accomplish a subtask.</li> <li>Prompts, are <code>System prompts</code> in the Generative AI context. They define what the LLM should do</li> </ul> <p>The following diagram presents, one agent that integrate one LLM, with a set of tools. One tool is helping to access a client given its name, queries a database, one tool is to compute the next best action is send structured data to a decision service, to get better decsion, and finally one is a retriever to access collections in a vector database to support Retrieved Augmented Generation use cases.</p> <p></p> <p>Tools can  be any python function, ot proxies to remote business service.</p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/#agents","title":"Agents","text":"<p>An agent is an interactive application or solution that supports a specific business use case, like helping a worker performing a specific task of a business process.  The execution of the agent involves the coordination of one or more LLMs.  Agents may be stateful to preserve the state of a conversation using snapshot capabilities. </p> <p>Agent management has two parts: 1/ the management of the OwlAgent entity definitions with REST resources and a persistence repository, and 2/ the agent runner instance which manages conversations:</p> <p></p> <p>In any solution the conversation manager service use one agent runner.</p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/#prompts","title":"Prompts","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#documents","title":"Documents","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#tools","title":"Tools","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#agentic-workflow","title":"Agentic workflow","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#a-customizable-agent-backend","title":"A customizable agent backend","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#backend-apis","title":"Backend APIs","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#hooks-to-custom-code","title":"Hooks to custom code","text":"<p>Coming soon</p>"},{"location":"Develop/1-Anatomy%20of%20a%20solution/#an-out-of-the-box-chatbot-frontend-webapp","title":"An out-of-the-box chatbot frontend webapp","text":""},{"location":"Develop/1-Anatomy%20of%20a%20solution/#how-the-frontend-and-backend-work-together","title":"How the frontend and backend work together","text":""},{"location":"Develop/2-Developers%20roadmap/","title":"Developer's roadmap to build a custom solution","text":"<p>This chapter addresses how to develop a custom solution and how to maintain or add new features to the current backend.</p>"},{"location":"Develop/2-Developers%20roadmap/#understand-which-approach-works-well-for-a-given-use-case","title":"Understand which approach works well for a given use case","text":"<p>LLMs are very good at providing linguistic capabilities such as:</p> <ul> <li>extracting data from text  </li> <li>translation  </li> <li>intent detection  </li> <li>sentiment analysis  </li> <li>summarization  </li> </ul> <p>On the other hand, LLMs are completely unable to reason in a logical, trustable, predictable or explainable manner.  There are situations where it is key to rely on robust symbolic reasoning capabilities in order to: </p> <ul> <li>deduce new facts from known facts  </li> <li>decide in accordance to company policies  </li> </ul> <p>If you need to take business decisions that must be trusted from a risk and compliance perspective, it would be completely unreasonable to  place a statistical bet on the fact that a linguistic parrot can find the right answer.</p> <p>The right approach is to use the right technology for the right use cases. The Athena Owl Agent framework will make your life easy at integrating various approaches to build an hybrid AI solution.</p>"},{"location":"Develop/2-Developers%20roadmap/#choose-the-right-ingredients","title":"Choose the right ingredients","text":"<p>The following table shows various features that are used by three different applications built via the Athena Owl Agent framework.  Depending on the capabilities we want to provide to the employees that will interact with the agent, we will leverage different features. </p> Simple agent with RAG Insurance complaint handling Tax assistant LLM general Q&amp;A extract data, detect intention extract data, detect intention, generate summary of decision outcome RAG queries on company documents queries on company policies Fetch data (tool calling to data APIs) fetch customer and claim data fetch tax payer and vehicle data Decide based on company policies (tool calling to rule service) determine next best action determine eligibility for a tax discount Human in the loop, Open/Closed conversation ask specific questions needed to decide"},{"location":"Develop/2-Developers%20roadmap/#create-a-solution-folder-using-a-template","title":"Create a solution folder using a template","text":"<ul> <li>Install Python 3.11 if needed</li> <li>Run <code>owl-solution-create</code> script</li> </ul>"},{"location":"Develop/2-Developers%20roadmap/#create-a-python-virtual-environment","title":"Create a Python virtual environment","text":"<pre><code>cd $DEMO/ibu_backend/src\npython3 -m venv .venv\nsource .venv/bin/activate\n</code></pre>"},{"location":"Develop/2-Developers%20roadmap/#install-dependencies","title":"Install dependencies","text":"<p>Next, we need to install the Python library dependencies and set the Python path. <pre><code>pip install -r requirements.txt\ncd .. \nsource setpython.sh\n</code></pre></p>"},{"location":"Develop/2-Developers%20roadmap/#declare-the-various-elements-of-the-solution","title":"Declare the various elements of the solution","text":"<p>Edit yaml files (or use admin console)</p>"},{"location":"Develop/2-Developers%20roadmap/#add-hooks-to-custom-code","title":"Add hooks to custom code","text":"<p>This is how we can declare a hook to override the default agent runner class <pre><code>ibu_tax_agent:\n  agent_id: ibu_tax_agent\n  name: Tax Agent\n  description: OpenAI based agent with tool to call tax reduction eligibility service\n  runner_class_name: ibu.llm.agents.IBU_TaxAgent.IBU_TaxAgent                            # custom runner class\n  class_name: athena.llm.agents.base_chain_agent.OwlAgent\n  modelClassName: langchain_openai.ChatOpenAI\n  modelName: gpt-3.5-turbo-0125\n  prompt_ref: tax_first_intent_prompt\n  temperature: 0\n  top_k: 1\n  top_p: 1\n</code></pre></p>"},{"location":"Develop/2-Developers%20roadmap/#setup-unit-and-integration-tests","title":"Setup unit and integration tests","text":"<p>To run all the unit tests: pytest -s tests/ut</p> <p>To run the integration tests: ...</p>"},{"location":"Develop/2-Developers%20roadmap/#implement-custom-code","title":"Implement custom code","text":"<p>We create a file ibu.llm.agents.IBU_TaxAgent.IBU_TaxAgent.py to implement the IBU_TaxAgent class <pre><code>from athena.llm.agents.agent_mgr import OwlAgentDefaultRunner, OwlAgent, get_agent_manager\n\nclass IBU_TaxAgent(OwlAgentDefaultRunner):\n    # your custom code here\n</code></pre></p> <p>Implement an agentic workflow using Langgraph</p>"},{"location":"Develop/2-Developers%20roadmap/#run-tests","title":"Run tests","text":""},{"location":"Develop/2-Developers%20roadmap/#exploratory-testing","title":"Exploratory testing","text":"<p>There are two possible ways to run the out-of-the-box frontend webapp:</p> <ul> <li>Build and run a Docker image for the frontend so that it will be started by <code>docker compose up -d</code> script</li> <li>Run the chatbot webapp locally using node (version &gt;= 18.18) and yarn</li> </ul> <p>If needed, install node:  <pre><code>nvm install --lts\nnvm use --lts\n</code></pre></p> <p>If needed, install yarn: <code>npm install --global yarn</code></p> <pre><code>cd owl-agent-interface\nyarn\nyarn dev\n</code></pre>"},{"location":"Develop/2-Developers%20roadmap/#unit-testing","title":"Unit testing","text":"<p>To run all the unit tests: <pre><code>pytest -s tests/ut\n</code></pre></p>"},{"location":"Develop/2-Developers%20roadmap/#integration-testing","title":"Integration testing","text":"<p>Before running the integration tests, we need to start the Docker containers: <pre><code>cd $DEMO/deployment/local\ndocker compose up -d\n</code></pre></p> <p>We can then run all the integration tests: <pre><code>pytest -s tests/it\n</code></pre></p>"},{"location":"Develop/2-Developers%20roadmap/#package","title":"Package","text":""},{"location":"Develop/2-Developers%20roadmap/#deliver-solution","title":"Deliver solution","text":""},{"location":"Develop/3-Tutorials/","title":"Overview of tutorials","text":""},{"location":"Develop/3-Tutorials/#build-an-ai-app-bootcamp","title":"Build an AI App Bootcamp","text":"<ul> <li> Bootstrap your AI app project</li> <li> Leverage data and decision APIs</li> <li> Leverage documents with RAG</li> <li> Add human in the loop</li> <li> Create new backend API endpoints</li> </ul>"},{"location":"Develop/3-Tutorials/#use-various-providers","title":"Use various providers","text":"<ul> <li>Local LLM</li> <li>Mistral</li> <li>OpenAI</li> <li>IBM Granite</li> </ul>"},{"location":"Develop/3-Tutorials/#frontend","title":"Frontend","text":"<ul> <li>Customize the chatbot frontend webapp</li> <li>Integrate the Owl chatbot widget</li> </ul>"},{"location":"Develop/3-Tutorials/Backend/Bootstrap%20an%20app/","title":"Tutorial - Bootstrap your AI app project","text":"<p>In this tutorial, we will guide you through all the steps needed to create a new AI app with the Athena Owl Framework.</p> <p>We will: - create a new AI app project from a template - run the AI app so that you can understand the various ways to interact with the app - modify the AI app to add a new agent with its own prompt and a custom tool</p>"},{"location":"Develop/3-Tutorials/Backend/Bootstrap%20an%20app/#prerequisites","title":"Prerequisites","text":"<p> You will need to be able to run Docker on your machine. The following instructions have been built using Docker Desktop but you can use alternative tools such as Colima or Rancher Desktop.</p>"},{"location":"Develop/3-Tutorials/Backend/Bootstrap%20an%20app/#create-a-new-agent-app-from-a-template","title":"Create a new Agent App from a template","text":"<p>Create a working directory for everything related to Athena and move into it. From a terminal, you can do: <pre><code>mkdir AthenaDecisionSystems\ncd AthenaDecisionSystems\n</code></pre></p> <p>Clone athena-owl-core and athena-owl-demos github repositories <pre><code>git clone https://github.com/AthenaDecisionSystems/athena-owl-core.git\ngit clone https://github.com/AthenaDecisionSystems/athena-owl-demos.git\n</code></pre> Alternatively and if you have an SSH key installed for github, you can do this: <pre><code>git clone git@github.com:AthenaDecisionSystems/athena-owl-core.git \ngit clone git@github.com:AthenaDecisionSystems/athena-owl-demos.git\n</code></pre> The athena-owl-core repository contains the code of the Athena backend (implemented using FastAPI) and frontend component (implemented as a single page application using React). As developers, we will see how we can build a custom app. We will be able to reuse some code, for instance Python functions from the backend.</p> <p>Create a folder that will be the placeholder for your first Athena project:  <pre><code>mkdir my-athena-ai-app\n</code></pre> Recommendation: place the 'my-athena-ai-app' folder directly under the AthenaDecisionSystems folder (so at the same level as the 'athena-owl-core' folder) on your disk. We provide some scripts that rely on relative paths and they will be easier to use.</p> <p>Copy the content of the skeleton app SkeletonAppHelloLLM into your app folder</p> <pre><code>cp -a athena-owl-demos/SkeletonApp-HelloLLM/. my-athena-ai-app/\n</code></pre> <p>Create a .env file in the app folder Add the API keys for the various third party providers that you want to use.  <pre><code>cd my-athena-ai-app\nvi .env\n</code></pre></p> <p>In this demo, we use OpenAI and Tavily. So, the .env file should look like this: <pre><code>OPENAI_API_KEY=&lt;your OpenAI key here&gt;\nTAVILY_API_KEY=&lt;your Tavily key here&gt;\n</code></pre> Please insert your OpenAI and Tavily API keys in the .env file if you already have the keys.  Otherwise, follow these links to create keys for OpenAI and Tavily</p> <p>We are now ready to run our application. First, make sure that Docker Desktop is started and then use this command to start the demo with Docker Compose: <pre><code>cd deployment/local\ndocker compose up -d\n</code></pre></p> <p>This will pull the two Docker images that are used to run our application:</p> <ul> <li> <p>athenadecisionsystems/athena-owl-backend is the backend component that will serve APIs</p> </li> <li> <p>athenadecisionsystems/athena-owl-frontend is the OOTB web application that can be used to interact with AI agents using a chatbot interface.</p> </li> </ul> <p>Once the images have been pulled, you can check that two containers are actually up and running: <pre><code>docker ps\n</code></pre> The output of the command should look like:  <pre><code>CONTAINER ID   IMAGE                                             COMMAND                  CREATED          STATUS          PORTS                    NAMES\nd1bee478ae99   athenadecisionsystems/athena-owl-backend:1.0.0    \"uvicorn athena.main\u2026\"   11 seconds ago   Up 10 seconds   0.0.0.0:8002-&gt;8000/tcp   ibu-backend\n35530e7bd690   athenadecisionsystems/athena-owl-frontend:1.0.0   \"docker-entrypoint.s\u2026\"   6 days ago       Up 10 seconds   0.0.0.0:3000-&gt;3000/tcp   owl-frontend\n</code></pre></p>"},{"location":"Develop/3-Tutorials/Backend/Bootstrap%20an%20app/#run-the-default-hello-llm-agent-app","title":"Run the default 'Hello LLM' Agent App","text":"<p>We are now ready to interact with our Hello World AI application. Running our Agent App is a good way to understand how we can interact with it, either using an out-of-the-box chatbot provided as a webapp or by calling APIs.</p> <p>Start a browser and point to localhost:3000</p> <ul> <li> <p>we will use the OOTB chatbot to send queries that will be served by the pre-configured LLM. Click on the Chatbot tab in the top navigation bar. In the chat, enter the following message: <pre><code>What is the capital of Italy?\n</code></pre> The agent will use the common knowledge of an LLM like OpenAI to provide an answer. A reasonable answer should look like: <pre><code>The capital of Italy is Rome.\n</code></pre></p> </li> <li> <p>The agent has been configured with some tools and will use a specific tool to retrieve customer data based on their email address. If you enter the following query: <pre><code>What are the details of peter@acme.com?\n</code></pre> You will get the following response: <pre><code>Here are the details for the client with the email \"peter@acme.com\":\n    Email: peter@acme.com\n    Date of Birth: December 14, 1994\n    Income: $19,500 USD\n    Country of Residence: United States\n</code></pre> The response is returned by a custom function that mimicks a call to an enterprise data API.</p> </li> <li> <p>The agent can also use Tavily to search the web for all queries that require fresh data that was not available when the LLM was trained. <pre><code>what is the weather today in London? \nplease also provide your source in terms of web url\n</code></pre> The agent will perform an API call to Tavily. A reasonable answer should look like: <pre><code>The current weather in London is as follows:\n\n    Temperature: 7.2\u00b0C (45.0\u00b0F)\n    Condition: Light drizzle\n    Wind: 5.4 mph (8.6 kph) from the northeast\n    Humidity: 100%\n    Pressure: 1029.0 mb\n    Visibility: 6.0 km (3.0 miles)\n    Feels Like: 5.6\u00b0C (42.0\u00b0F)\n\nThis information is sourced from WeatherAPI.\n</code></pre></p> </li> </ul> <p>Play with the Athena Backend APIs using Swagger UI Start a browser and point to localhost:8002/docs Use the generic_chat endpoint with the following JSON payload <pre><code>{\n  \"locale\": \"en\",\n  \"query\": \"what is the data of pierre@acme.fr?\",\n  \"user_id\": \"123\",\n  \"agent_id\": \"hello_world_agent_with_tools\"\n}\n</code></pre> The response should look like this <pre><code>{\n  \"messages\": [\n    {\n      \"content\": \"The data for the email address pierre@acme.fr is as follows:\\n\\n- **Date of Birth**: December 14, 1994\\n- **Income**: 19,500 EUR\\n- **Country of Residence**: France\",\n      \"style_class\": null\n    }\n  ],\n  \"closed_questions\": null,\n  \"reenter_into\": null,\n  \"status\": 200,\n  \"error\": \"\",\n  \"chat_history\": [\n    {\n      \"content\": \"what is the data of pierre@acme.fr?\",\n      \"role\": \"human\"\n    },\n    {\n      \"content\": \"The data for the email address pierre@acme.fr is as follows:\\n\\n- **Date of Birth**: December 14, 1994\\n- **Income**: 19,500 EUR\\n- **Country of Residence**: France\",\n      \"role\": \"AI\"\n    }\n  ],\n  \"user_id\": \"123\",\n  \"agent_id\": \"hello_world_agent_with_tools\",\n  \"thread_id\": \"8d3a7baf-b266-4f35-9a28-04edff0a5703\"\n}\n</code></pre></p> <p>If you want, you can also make direct calls to the Athena Backend APIs using cURL from a terminal. <pre><code>curl -X 'POST' \\\n  'http://localhost:8002/api/v1/c/generic_chat' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"locale\": \"en\",\n  \"query\": \"what is the data of pierre@acme.fr?\",\n  \"user_id\": \"123\",\n  \"agent_id\": \"hello_world_agent_with_tools\"\n}'\n</code></pre></p>"},{"location":"Develop/3-Tutorials/Backend/Bootstrap%20an%20app/#create-your-own-agent-app","title":"Create your own Agent App","text":""},{"location":"Develop/3-Tutorials/Backend/Bootstrap%20an%20app/#create-a-new-prompt","title":"Create a new prompt","text":"<p>Edit the prompts.yaml file under ...</p>"},{"location":"Develop/3-Tutorials/Backend/Bootstrap%20an%20app/#create-a-new-agent","title":"Create a new agent","text":"<p>Edit the agents.yaml file</p>"},{"location":"Develop/3-Tutorials/Backend/Bootstrap%20an%20app/#add-a-new-tool","title":"Add a new tool","text":"<p>Register a new function as a tool in the tool factory</p>"},{"location":"Develop/3-Tutorials/Backend/Human%20in%20the%20loop/","title":"Tutorial - Managing a human in the loop","text":""},{"location":"Develop/3-Tutorials/Backend/Human%20in%20the%20loop/#another-heading","title":"Another heading","text":"<p>Coming soon</p>"},{"location":"Develop/3-Tutorials/Backend/RAG/","title":"Tutorial - RAG","text":""},{"location":"Develop/3-Tutorials/Backend/RAG/#another-heading","title":"Another heading","text":"<p>Coming soon</p>"},{"location":"Develop/3-Tutorials/Backend/Tool%20calling/","title":"Tutorial - Add tool calling","text":""},{"location":"Develop/3-Tutorials/Backend/Tool%20calling/#another-heading","title":"Another heading","text":"<p>Coming soon</p>"},{"location":"Develop/4-Reference/Backend%20APIs/","title":"Owl Backend REST APIs","text":"<p>Coming soon</p>"},{"location":"Develop/4-Reference/Owl%20core%20library/","title":"Owl Core Python library","text":"<p>Coming soon</p>"},{"location":"Get%20started/1-Run%20a%20demo/","title":"Run a demo of Athena AI","text":""},{"location":"Get%20started/1-Run%20a%20demo/#run-a-demo-on-your-machine-in-5-minutes","title":"Run a demo on your machine in 5 minutes","text":"<ul> <li> Run on Mac</li> <li> Run on Windows (Coming soon) </li> </ul>"},{"location":"Get%20started/1-Run%20a%20demo/#play-with-a-hosted-demo","title":"Play with a hosted demo","text":"<ul> <li> Try our SaaS demo (Coming soon)</li> </ul>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/","title":"Guide to run the IBU Insurance demo","text":"<p>This guide provides step-by-step instructions to set up this demo on macOS.</p>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#table-of-contents","title":"Table of Contents","text":"<ul> <li>macOS Installation</li> <li>Step 1: Install Homebrew</li> <li>Step 2: Install Colima</li> <li>Step 3: Install Docker and Docker Compose</li> <li>Step 4: Start Colima</li> <li>Step 5: Clone the Demo Repository</li> <li>Step 6: Setup IBM watsonx.ai</li> <li>Step 7: Setup the Demo environment</li> <li>Step 8: Run the Demo</li> <li>Step 9: Demo scenario</li> </ul>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#macos-installation","title":"macOS Installation","text":""},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-1-install-homebrew","title":"Step 1: Install Homebrew","text":"<p>Homebrew is a package manager for macOS that simplifies the installation of software.</p> <p>To install Homebrew, open your Terminal and run the following command:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>After the installation is complete, verify that Homebrew is installed:</p> <pre><code>brew --version\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-2-install-colima","title":"Step 2: Install Colima","text":"<p>Colima is a lightweight container runtime for macOS that works with Docker.</p> <p>To install Colima, run:</p> <pre><code>brew install colima\n</code></pre> <p>If you have an Apple Silicon (M1/M2) Mac, you need to install Rosetta:</p> <pre><code>softwareupdate --install-rosetta\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-3-install-docker-and-docker-compose","title":"Step 3: Install Docker and Docker Compose","text":"<p>Docker is essential for containerization, and Docker Compose helps in managing multi-container applications.</p> <p>To install Docker and Docker Compose, run:</p> <pre><code>brew install docker docker-compose\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-4-start-colima","title":"Step 4: Start Colima","text":"<p>Now, you can start Colima with the desired configuration. If you have an Intel Mac, run:</p> <pre><code>colima start --cpu 4 --memory 8\n</code></pre> <p>For Apple Silicon (M1/M2) Macs, run:</p> <pre><code>colima start --cpu 4 --memory 8 --vm-type=vz --vz-rosetta\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-5-clone-the-demo-repository","title":"Step 5: Clone the Demo Repository","text":"<p>Now that your environment is set up, you can clone the <code>athena-owl-demos</code> repository. If you don't have Git installed, you can do so by running:</p> <pre><code>brew install git\n</code></pre> <p>Then, clone the repository and navigate into the demo directory:</p> <pre><code>git clone https://github.com/AthenaDecisionSystems/athena-owl-demos.git\ncd athena-owl-demos\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-6-setup-ibm-watsonxai","title":"Step 6: Setup IBM watsonx.ai","text":"<ol> <li> <p>Create an IBM watsonx.ai account    Visit the IBM watsonx.ai page and follow the links to set up a Cloud instance.</p> </li> <li> <p>Generate an API key    In your IBM Cloud account, go to <code>Profile and settings</code> and generate an API key.</p> </li> <li> <p>Instantiate a watsonx.ai service    From your IBM Cloud account, create a new watsonx.ai service instance.</p> </li> <li> <p>Copy your watsonx.ai configuration parameters </p> </li> </ol> <pre><code>WATSONX_URL=&lt;your watsonx.ai URL&gt; # example value = https://us-south.ml.cloud.ibm.com/\nWATSONX_APIKEY=&lt;your watsonx.ai API Key&gt;\nWATSONX_PROJECT_ID=&lt;your watsonx.ai Project ID&gt;\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-7-setup-the-demo-environment","title":"Step 7: Setup the demo environment","text":"<p>Navigate to the IBU Insurance demo directory:</p> <pre><code>cd IBU-insurance-demo\n</code></pre> <p>Create your <code>.env</code> file. It contains some configuration parameters. Paste your watsonx.ai configuration parameters:</p> <pre><code>WATSONX_URL=&lt;your watsonx.ai URL&gt; # example value = https://us-south.ml.cloud.ibm.com/\nWATSONX_APIKEY=&lt;your watsonx.ai API Key&gt;\nWATSONX_PROJECT_ID=&lt;your watsonx.ai Project ID&gt;\n</code></pre> <p>If you plan to use other LLMs for which a key is needed, paste them here:</p> <pre><code># Define the IBM KEY to access models deployed on watsonx.ai\nIBM_API_KEY=USE_YOUR_IBM_API_KEY\n\n# Only when you want to use one of Open AI model\nOPENAI_API_KEY=USE_YOUR_OPENAI_API_KEY\n\n# Only when you want to use one a Mistral AI model\nMISTRAL_API_KEY=USE_YOUR_MISTRAL_API_KEY\n\n# Use Tavily to do search on last news, that could be interesting to validate tool calling\nTAVILY_API_KEY=USE_YOUR_TAVILY_API_KEY\n\n# if you want to get traces in Langchain tracing - this is optional\nLANGCHAIN_TRACING_V2=false\nLANGCHAIN_API_KEY=USE_YOUR_LANGCHAIN_KEY\n\n# If you want to use one of the Anthropic Claude models\nANTHROPIC_API_KEY=USE_YOUR_ANTHROPIC_KEY\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-8-run-the-demo","title":"Step 8: Run the Demo","text":"<p>Once everything is set up, you can run the project by executing:</p> <pre><code>cd deployment/local\ndocker-compose up -d\n</code></pre> <p>After the project is running, you can access the Owl Agent at:</p> <pre><code>http://localhost:3000\n</code></pre>"},{"location":"Get%20started/2-Install%20and%20run%20on%20Mac/#step-9-demo-scenario","title":"Step 9: Demo Scenario","text":"<p>Your environment should now be fully set up and ready for development and testing. If you encounter any issues, please refer to the documentation or raise an issue in the GitHub repository.</p>"},{"location":"Get%20started/3-Install%20and%20run%20on%20Windows/","title":"Install and run on Windows","text":"<p>Coming soon...</p>"},{"location":"Support/","title":"Getting support","text":"<p>Coming soon</p>"},{"location":"Technology/Deployment%20options/","title":"Athena Hybrid AI Technology","text":"<p>Coming soon</p>"},{"location":"Technology/Enterprise%20AI/","title":"Architecture for Enterprise AI","text":"<p>Coming soon</p>"},{"location":"Technology/Hybrid%20AI/","title":"Athena Hybrid AI Technology","text":"<p>Coming soon</p>"}]}